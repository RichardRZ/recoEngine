<html>

<head>
    <link rel="stylesheet" href="http://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Assignment 6: Twitter Sentiment Classification</title>
</head>

<body>

    <div class="row">
        <div class="col-md-2"></div>
        <div class="col-md-8">
            <div class="page-header center">
                <h1>Assignment 6 <small>Twitter Sentiment Classification</small></h1>
            </div>

              
            <h2></h2>

            <p>
                In this assignment I am attempting to use multiple machine learning techniques to teach a program to accurately classify tweets based solely on text. I will use Python, as well as the SciKit libraries, to parse a series of tweets and hopefully extract some information.
            </p>

            <p>
                The first step was tween processing. The main idea was to strip away any useless or extra information that could confuse the classifiers. Things like @ mentions, which will almost never tell us anything about the sentiment, links, and commmon english words are generally not helpful, so I removed them in addition to filtering out punctuation to reduce the feature space. I also attempted to remove more than two characters in a row, but I had performance issues and so left that aside. My features space is 53,551 features, I chose to stay with unigrams as I was concerned including bigrams would flood the feature space with mostly useless data while also causing a significant performance impact.
            </p>

            <p>
                Below are some statistics for each classifier:<br>
                <strong>NB</strong><br>
                Training accuracy: 83.9%<br>
                Cross-fold accuracy: 74.8%<br>
                Test accuracy: 81.3%<br>
                Avg. precision:81%<br>
                Avg. recall:81%<br>
                Avg. f1-score:81%<br>
                Confusion Matrix:<br>
                [[144  33]<br>
                [ 34 148]]<br>

                <img src="nb.png" style="width: 50%; height: 50%"><br>
                <br><br>

                <strong>Log</strong><br>
                Training accuracy: 86.04%<br>
                Cross-fold accuracy: 75.6%<br>
                Test accuracy: 80.5%<br>
                Avg. precision:81%<br>
                Avg. recall:81%<br>
                Avg. f1-score:80%<br>
                Confusion Matrix:<br>
                [[135  42]<br>
                 [ 28 154]]<br>

                <img src="log.png" style="width: 50%; height: 50%"><br>
                <br><br>
                <strong>SVM</strong><br>
                Training accuracy: 91.4%<br>
                Cross-fold accuracy: 73.7%<br>
                Test accuracy: 77.4%<br>
                Avg. precision:77%<br>
                Avg. recall:77%<br>
                Avg. f1-score:77%<br>
                Confusion Matrix:<br>
                [[132  45]<br>
                 [ 36 146]]<br><br>
                <img src="svm.png" style="width: 50%; height: 50%"><br>
      
            </p>
            <p>
                We can see the NB and Logistic regression classifiers ended up being fairly similar, although one interesting thing to note is that while the averages of precision and recall are close, the LOG classifier saw more variability which is confirmed by checking confusion matrix. It had higher precision in predicting a negative tweet sentiment, but lower recall, and vice versa for positive sentiments. It seems clear the the SVM classifier overfits, as it has a high training accuracy but a markedly lower test and cross fold accuracy than the other tests.
            </p> 

            <p>
                In the context of this assignment, precision refers to the percentage of the classifiers predictions actually aligned with the true label, as in how many of the tweets a classifier predicted as having a negative or positive sentiment actually had that sentiment. Recall is the ohter side of the coin, which is how many of the true labels was the classifier able to capture. It could simply pick a single tweet that it was sure had a positive sentiment, thus giving it 100% precision (for positive sentiments at least), but it's recall would be very poor as there are surely many more positive tweets than just one. The F1-score is mix of precision and recall, and gives a sense in a single measure as to how well our classifier does. Doing very well on one measure but poorly in another will be reflected in the F1 score. The confusion matrix gives a raw data look, as precision, recall, and the f1-score are essentially calculated from that matrix. Ideally every entry in the matrix except for the diagnoal vector (which corresponds the the classifier predicting the true label) would be 0. 
            </p>     
            NB ROC:<br>
            <img src="nb_roc.png" style="width: 50%; height: 50%"><br>
            LOG ROC:<br>
            <img src="log_roc.png" style="width: 50%; height: 50%"><br> 
            <p>
                We can see that the ROC plots for both the NB and LOG are similar, with the area under the curve in the high 80s. What the ROC curve essentially tells us is how much we have to pay (in terms of false positives) to get a certain true positive rate. Ideally, we would have 100% true positive for all false positive rates, but this is not feasible in reality. We can see that at around the 40% false positive rate there is a major decline in returns, and we cross the one to one threshold around 20% false positive (80% true positive).
            </p>

            <p>
                For the NB classifer, the top 20 most informative features for both positive and negative classification are as follow:

                <pre>
                        -30.9598    canceled            21.0000 smiles         
                        -28.0112    farrah              20.0000 blessed        
                        -27.0270    boooo               18.0000 poem           
                        -27.0270    toe                 14.5000 vip            
                        -25.9740    hates               13.0000 promoting      
                        -22.9885    gutted              12.6667 congratulations
                        -20.0000    itchy               12.3200 welcome        
                        -17.9856    booooo              12.0000 zac            
                        -17.9856    cancelled           11.5000 recommendation 
                        -17.9856    fawcett             11.0000 gem            
                        -17.8571    hurts               11.0000 howdy          
                        -17.5131    booo                11.0000 phrase         
                        -17.2117    bummed              11.0000 promote        
                        -17.0648    sad                 10.5000 deserved       
                        -16.0000    throat              10.3333 appreciated    
                        -15.1515    lonely              10.0000 appreciation   
                        -14.9925    crashes             10.0000 author         
                        -14.4718    died                10.0000 ciao           
                        -14.0056    depressing          10.0000 cozy           
                        -14.0056    homesick            10.0000 hottest  
                </pre>
            </p>   
            <p>
                For the LOG classifer, the top 20 most informative features for both positive and negative classification are as follow:

                <pre>
                        -3.1579 sad                 2.0950  welcome        
                        -2.8919 cancelled           2.0118  proud          
                        -2.7129 lonely              1.9679  congratulations
                        -2.6722 hates               1.9113  blessed        
                        -2.6460 misses              1.8815  thank          
                        -2.5970 bummed              1.8650  happily        
                        -2.5777 gutted              1.8626  smiling        
                        -2.4817 sadly               1.8505  phrase         
                        -2.4428 depressing          1.8269  sub            
                        -2.4234 canceled            1.8041  smile          
                        -2.4188 bummer              1.8016  flirting       
                        -2.3712 unfortunately       1.7771  hehehe         
                        -2.3632 leaves              1.7370  yayyy          
                        -2.3588 sucks               1.7319  thankful       
                        -2.3421 died                1.7018  congrats       
                        -2.3029 hurts               1.6846  chillin        
                        -2.2908 fml                 1.6822  pounds         
                        -2.2860 disappointed        1.6798  yaaaaay        
                        -2.2552 upset               1.6537  vip            
                        -2.2518 ruined              1.6412  tyler
                </pre>
            </p>   
            <p>
                For the SVM classifer, the top 20 most informative features for both positive and negative classification are as follow:

                <pre>
                        -2.1261 sauna               2.4926  brutus         
                        -2.0493 phn                 2.4376  petunias       
                        -1.9404 canr                2.4180  oooofff        
                        -1.9115 shawneda            2.1337  tourniquets    
                        -1.8915 robb                2.1166  bch            
                        -1.8910 fp2                 2.1141  howdy          
                        -1.8387 trunk               2.1013  cornell        
                        -1.7838 sod                 2.0719  dome           
                        -1.7834 headshot            2.0341  genuinely      
                        -1.7817 tonightso           2.0041  insome         
                        -1.7539 diyquot             1.9571  hartsfield     
                        -1.7485 pinas               1.9459  deans          
                        -1.7432 aaaaaaaaaw          1.9186  neechan        
                        -1.7193 aghh                1.9183  wifequot       
                        -1.7137 tics                1.9181  bubblewrap     
                        -1.6933 eagerly             1.9125  sleepingi      
                        -1.6923 waitdoes            1.9105  caturday       
                        -1.6816 deric               1.9028  brockenhurst   
                        -1.6812 nooooooooo          1.8803  brizzle        
                        -1.6793 doorman             1.8503  tcb 
                </pre>
            </p>

            <p>
                Looking at the list of most informative features, it is not surprising that the SVM performed worse, and had such a wide discrepancy between the training and test accuracy. Whereas the Log and NB classifier word list had words that, independent of context, would mostly give a good hint as to the sentiment, the SVM contained words like "tics", "canr", and "sauna". These seem to indicat overfitting to a particular set of data.
            </p>  

            <p>
                Overall, its a close call between the Log and NB classifers as to which performed the best. Log had better scores on training data, but did slightly worse on the test data. Interestingly, the most informative features for LOG are subjectively more in line with sentiments, using more generalizable words and lacking features like "farrah fawcett", which are certainly anomalous to when the data was collected. However, the NB classifier was more consistent, and ultimately did better in testing, making it the better classifier in this case.
            </p> 

            <p>
            The highest five predicted tweets that were correctly classified are as follows:

            <pre>
            "I hate Time Warner! Soooo wish I had Vios. Cant watch the fricken Mets game w/o buffering. I feel like im watching free internet porn." - .9999, negative sentiment

            " U WON'T FEEL ANY DISCOMORT! PROB WON'T EVEN NEED PAIN PILLS"" MAN U TWIPPIN THIS SHIT HURT!! HOW MANY PILLS CAN I TAKE!!" - .9999, negative sentiment

            "F*ck Time Warner Cable!!! You f*cking suck balls!!! I have a $700 HD tv &amp; my damn HD channels hardly ever come in. Bullshit!!" - .9999, negative sentiment

            "@springsingfiend @dvyers @sethdaggett @jlshack AT&amp;T dropped the ball and isn't supporting crap with the new iPhone 3.0... FAIL #att SUCKS!!!"  - .9999, negative sentiment

            "@sklososky Thanks so much!!! ...from one of your *very* happy Kindle2 winners ; ) I was so surprised, fabulous. Thank you! Best, Kathleen" - .9999, positive sentiment
            </pre>
            </p>

            <p>
            The highest five predicted tweets that were incorrectly classified are as follows:

            <pre>
            "My wrist still hurts. I have to get it looked at. I HATE the dr/dentist/scary places. :( Time to watch Eagle eye. If you want to join, txt!" - .9996, predicted negative sentiment, actually positive

            "is studing math ;) tomorrow exam and dentist :)" - .9922, predicted negative sentiment, actually positive

            "Wat the heck is North Korea doing!!??!! They just conducted powerful nuclear tests! Follow the link: http://www.msnbc.msn.com/id/30921379" - .9989, predicted positive sentiment, actually negative

            "My dad was in NY for a day, we ate at MESA grill last night and met Bobby Flay. So much fun, except I completely lost my voice today."  - .9611, predicted negative sentiment, actually positive

            "Back from seeing 'Star Trek' and 'Night at the Museum.' 'Star Trek' was amazing, but 'Night at the Museum' was; eh." - .9574, predicted positive sentiment, actually negative
            </pre>
            </p>



			

        <div class="col-md-2"></div>
    </div>

</body>
</html>